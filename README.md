# MambaDiff
Abstract:

In this work, we introduce MambaDiff, a novel architecture that integrates the strengths of diffusion models with the Mamba architecture to address the challenges faced in sequence-to-sequence (Seq2Seq) modeling tasks. Traditional Seq2Seq models, while powerful, often struggle with maintaining contextual relevance and computational efficiency, especially over longer sequences. By leveraging the gradual refinement capabilities of diffusion models and the scalable, efficient processing of the Mamba architecture, MambaDiff aims to enhance the quality of generated text across extensive contexts significantly. We demonstrate the potential of our approach through diverse NLP tasks such as open-domain dialogue, question generation, text simplification, and paraphrasing. This paper will detail the motivation behind this integration, the theoretical and practical benefits, and preliminary results that highlight its effectiveness compared to existing models.

See our project poster here:
[MambaDiffPoster.pdf](https://github.com/XiyahC/MambaDiff/blob/67814036f82558d9464cdcc370d84ab57117b45a/MambaPoster.pdf)

See our project report here:
[MambaDiff: Revolutionizing Seq2Seq Models with Diffusion model and Mamba architecture](https://github.com/XiyahC/MambaDiff/blob/d1a08d0af1a84309b7f7bb1d5c5c3c461bc7d749/Final_Report_MambaDiff__Revolutionizing_Seq2Seq_Models_with_Diffusion_Model_and_Mamba_Architectures.pdf)
